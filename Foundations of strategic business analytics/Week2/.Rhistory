nasa_fireball$vel[which.max(nasa_fireball$vel)]
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
summary(nasa_fireball$impact_e)
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
install.packages("leaflet")
library(leaflet)
library(leaflet)
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
library(nycflights13)
library(tidy\)
library(tidyverse)
library(tidyverse)
flights
library(nycflights13)
library(tidyverse)
View(flights)
filghts %>% filter(month==1, day==1)
flights %>% filter(month==1, day==1)
filter(filghts, month==1, day==1)
filter(flights, month==1, day==1)
flights %>% filter(month==11 | month==12)
library(nycflights13)
library(tidyverse)
View(flights)
# filter
flights %>% filter(month==1, day==1)
flights %>% filter(month==11 | month==12)
filter(flights, month==1, day==1)
# arrange -> change the order
flights %>% arrange(desc(year))
flights %>% arrange(asc(year))
flights %>% arrange(year)
unique(flights$year)
unique(flights$month)
flights %>% arrange(month)
flights %>% arrange(desc(month))
flights %>% arrange(desc(month), desc(day))
flights %>% select(year, month, day)
flights %>% select(-year)
flights$carrier %>% select(contains("UA"))
flights %>% select(contains("UA"))
names(flights)
mutate(unite(flights, date, year, month, day, sep = "-"))
a <- mutate(unite(flights, date, year, month, day, sep = "-"))
View(a)
library(lubridate)
a <- mutate(unite(flights, date, year, month, day, sep = "-"))
a$date <- as.Date(a$date)
View(a)
a <- mutate(unite(flights, date, year, month, day, sep = "-"),
flight_new <- flights$flight/100)
a$date <- as.Date(a$date)
View(a)
a <- mutate(unite(flights, date, year, month, day, sep = "-"),
flight_new = flight/100)
a$date <- as.Date(a$date)
View(a)
x <- 1:10
cumsum(x)
flights
flights %>% group_by(carrier) %>% summarize(count = n())
shiny::runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/ref3')
shiny::runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('~/Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
shiny::runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
shiny::runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard')
runApp('Desktop/Temp_Shiny')
LETTERS
sample(LETTERS, 5)
runApp('Desktop/Temp_Shiny')
?selectInput
runApp('Desktop/Temp_Shiny')
region_list <- c("APAC", "NAM", "LATAM", "EMEA")
lapply(region_list, function(x){paste("Total Mailbox -", x)})
runApp('Desktop/Temp_Shiny')
strong(paste0('Hi, this is output B#', i))
runApp('Desktop/Temp_Shiny')
mtcars
mtcars.info()
mtcars.describe()
sum(mtcars$cyl)
sum(is.na(mtcars$cyl))
unique(mtcars$vs)
paste0("m_",1)
fluidRow(
box(valueBoxOutput("mailbox_a", width = 6),
title="Total Mailbox - APAC" ,width = 3,background = 'teal'),
box(valueBoxOutput("mailbox_n", width = 6),
title="Total Mailbox - NAM" ,width = 3,background = 'teal'),
box(valueBoxOutput("mailbox_l", width = 6),
title="Total Mailbox - LATAM" ,width = 3,background = 'teal'),
box(valueBoxOutput("mailbox_e", width = 6),
title="Total Mailbox - EMEA" ,width = 3,background = 'teal')
)
fluidRow(
lapply(c("APAC", "NAM", "LATAM", "EMEA"), function(i){box(valueBoxOutput(paste0("m_",i), width=6),
title = paste("Total Mailbox -", i), width = 3, background = "teal")})
)
res <- lapply(1:5, function(i) input[[paste0('a', i)]])
?setNames
runApp('Desktop/Temp_Shiny')
installed.packages("shinydashboardPlus")
library(shinydashboardPlus)
installed.packages("shinydashboardPlus")
fluidRow(
lapply(0:1, function(i){box(valueBoxOutput(paste0("m_",i), width=6),
title = paste("Total Mailbox -", i), width = 3, background = "teal")})
)
lapply(1:2, function(x){
output[[paste0('m_',x)]] <- renderText({
data <- mtcars
data <- mtcars %>% filter(vs==x)
return(dim(data)[1])
})
})
output$mailbox_n <- renderText({
data <- data_m
data <- data %>% filter(!is.na(MailboxSizeGB), Region=="NAM")
return(dim(data)[1])
})
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard')
mtcars
str(mtcars)
sum(is.na(mpg))
sum(is.na(mtcars$mpg))
sum(is.na(mtcars$cyl))
sum(is.na(mtcars$vs))
sum(is.na(mtcars$carb))
sum(is.na(mtcars$gear))
sum(is.na(mtcars$hp))
library(readxl)
data <- read.csv("~/Documents/GitHub/DataCamp_R/Datasets/census.csv")
str(data)
sum(is.na(data$JAN))
sum(is.na(data$OCT))
sum(is.na(data))
library(readxl)
data <- load("~/Documents/GitHub/DataCamp_R/Datasets/weather.rds")
weather <- readRDS("~/Documents/GitHub/DataCamp_R/Datasets/weather.rds")
sum(is.na(data))
library(readr)
data <- read.csv("~/Downloads/GitHub/DataCamp_R/Datasets/students.csv")
library(readr)
data <- read.csv("~/Downloads/GitHub/DataCamp_R/Datasets/students.csv")
library(readr)
data <- read.csv("~/Downloads/GitHub/DataCamp_R/Datasets/students.csv")
library(readr)
data <- read.csv("~/Documents/GitHub/DataCamp_R/Datasets/students.csv")
View(data)
unique(data$address)
unique(data$Pstatus)
unique(data$Medu)
data$Medu[data$Medu=="4"] <- NA
data$Medu[data$Medu=="3"] <- "abc"
unique(data$Medu)
if (is.na(data$Medu)){
data$Medu[is.na(data$Medu)] <- 4
} else if(data$Medu=="abc"){
data$Medu[is.na(data$Medu)] <- 4
} else{
data$Medu <- data$Medu
}
unique(data$Medu)
# Learn R program to apply a function for each row in r data frame
# R Data Frame
celebrities = data.frame(name = c("Andrew", "Mathew", "Dany", "Philip", "John", "Bing", "Monica"),
age = c(28, 23, 49, 29, 38, 23, 29),
income = c(25.2, 10.5, 11, 21.9, 44, 11.5, 45))
# R function
f = function(x, output) {
# x is the row of type Character
# access element in first column
name = x[1]
# access element in second column
income = x[3]
#your code to process x
cat(name, income, "\n")
}
#apply(X, MARGIN, FUN, …)
apply(celebrities, 1, f)
celebrities = data.frame(name = c("Andrew", "Mathew", "Dany", "Philip", "John", "Bing", "Monica"),
age = c(28, 23, 49, 29, 38, 23, 29),
income = c(25.2, 10.5, 11, 21.9, 44, 11.5, 45))
# R function
f = function(x$name, output) {
# x is the row of type Character
# access element in first column
name = x[1]
# access element in second column
income = x[3]
#your code to process x
cat(name, income, "\n")
}
#apply(X, MARGIN, FUN, …)
apply(celebrities, 1, f)
data <- apply(data$Medu, 1, function(x){
if(is.na(x$Medu)){
x$Medu[is.na(x$Medu)] <- 4
}else if(x$Medu == "abc"){
x$Medu[x$Medu=="abc"] <- 3
}else{
x$Medu
}
})
data <- apply(data, 1, function(x){
if(is.na(x$Medu)){
x$Medu[is.na(x$Medu)] <- 4
}else if(x$Medu == "abc"){
x$Medu[x$Medu=="abc"] <- 3
}else{
x$Medu
}
})
data$Medu <- apply(data$Medu, 1, function(x){
if(is.na(x)){
x[is.na(x)] <- 4
}else if(x == "abc"){
x[x=="abc"] <- 3
}else{
x
}
})
data$Medu <- ifelse(is.na(data$Medu), 4, data$Medu)
unique(data$Medu)
data$Medu <- ifelse(is.na(data$Medu), 4, data$Medu)
data$Medu <- ifelse(data$Medu=="abc", 3, data$Medu)
unique(data$Medu)
data$Medu[data$Medu=="4"] <- "10 GB"
data$Medu[data$Medu=="3"] <- "1000 KB"
data$Medu <- ifelse(data$Medu=="10 GB", str_extract(data$Medu, [0-9]{1,5}), data$Medu)
data$Medu <- ifelse(data$Medu=="10 GB", str_extract(data$Medu, "[0-9]{1,5}"), data$Medu)
library(stringr)
data$Medu <- ifelse(data$Medu=="10 GB", str_extract(data$Medu, "[0-9]{1,5}"), data$Medu)
unique(data$Medu)
data$Medu <- ifelse(data$Medu=="10 GB", trimws(str_extract(data$Medu, "[0-9]{1,5}")), data$Medu)
unique(data$Medu)
data$Medu <- ifelse(data$Medu=="1000 KB", trimws(str_extract(data$Medu, "[0-9]{1,5}")), data$Medu)
unique(data$Medu)
str_extract("0.1 GB", [0-9]{1,5})
str_extract("0.1 GB", "[0-9]{1,5}")
grep(pattern = "(0\\.[0-9]+)", "0.1 GB" ,value = T)
grep(pattern = "(0\\.[0-9]{1,4})", "0.1 GB" ,value = T)
grep(pattern = "(\\.[0-9]{1,4})", "0.1 GB" )
grep(pattern = "([0-9]{1,4})?(\\.[0-9]{1,4})", "0.1 GB" )
regexpr("([0-9]{1,4})?(\\.[0-9]{1,4})", "0.1 GB")
str_extract("24 GB", "\\d+\\.*\\d*")
str_extract("24.15 GB", "\\d+\\.*\\d+")
str_extract("24.15 GB", "\\d+\\.*\\d*")
unique(data$Medu)
data$Medu[data$Medu=="10"] <- "0.15 GB"
data$Medu[data$Medu=="1000"] <- "0.1 GB"
unique(data$Medu)
data$Medu <- ifelse(data$Medu=="0.15 GB", trimws(str_extract(data$Medu, "\\d+\\.*\\d+")), data$Medu)
data$Medu <- ifelse(data$Medu=="0.1 GB", trimws(str_extract(data$Medu, "\\d+\\.*\\d+")), data$Medu)
unique(data$Medu)
R.version
install.packages("swirl")
library(swirl)
install_course("Advance R Programming")
swirl::install_course("Advanced R Programming")
swirl()
x <- runif(1, 0, 10)
x
if(x > 3) {
y <- 10
} else {
y <- 0
}
y
x <- matrix(1:6, 2, 3)
x
nrow(x)
ncol(x)
for(i in seq_len(nrow(x))){print(i)}
for(i in seq_len(nrow(x))) {
for(j in seq_len(ncol(x))) {
print(x[i, j])
}
}
for(i in seq_len(ncol(x))){print(i)}
x
gwd
getwd
getwd()
if(!file.exists("data/2016-07-20.csv.gz")) {
download.file("http://cran-logs.rstudio.com/2016/2016-07-20.csv.gz",
"data/2016-07-20.csv.gz")
}
if(!file.exists("data/2016-07-20.csv.gz")) {
download.file("http://cran-logs.rstudio.com/2016/2016-07-20.csv.gz",
"data/2016-07-20.csv.gz")
}
cran <- read_csv("data/2016-07-20.csv.gz", col_types = "ccicccccci")
library(readr)
library(dplyr)
cran <- read_csv("data/2016-07-20.csv.gz", col_types = "ccicccccci")
cran <- read_csv("data/2016-07-20.csv.gz", col_types = "ccicccccci")
cran <- read_csv("data/2016-07-20.csv.gz", col_types = "ccicccccci")
cran %>% filter(package == "filehash") %>% nrow
swirl()
Sys.Date()
mean(c(2,4,5))
submit()
boring_function('My first function!')
boring_function()
boring_function
submit()
my_mean(c(4,5,10))
submit()
remainder(5)
remainder(11,5)
remiander(divisor=11, num=5)
remainder(divisor = 11, num=5)
remainder(4, div=2)
args(remainder)
submit()
submit()
evaluate(sd, c(1.4, 3.6, 7.9, 8.8))
evaluate(function(x){x+1},6)
evaluate(function(x){x[1]}, c(8,4,0))
evaluate(function(x){x[3]}, c(8,4,0))
evaluate(function(x){x[length(x)]}, c(8,4,0))
?paste
paste("Programming", "is", "fun!")
submit()
paste("START", "hi")
submit()
telegram("Good", "morning")
submit()
mad_libs("US", "car")
submit()
"I" %p% "love" %p% "R!"
swirl()
Sys.Date()
swilr()
swirl()
adder_maker <- function(n){
function(x){
n + x
}
}
add2 <- adder_maker(2)
add3 <- adder_maker(3)
add2(5)
add3(5)
library(purrr)
map_chr(c(5, 4, 3, 2, 1), function(x){
c("one", "two", "three", "four", "five")[x]
})
map_lgl(c(1, 2, 3, 4, 5), function(x){
x > 3
})
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
set.seed(1234)
library(ggplot2)
library(lattice)
set.seed(1234)
library(ggplot2)
library(lattice)
dbern(1,0.85)
dbinom(100, 120, 0.85)
pbinom(100, 120, 0.85)
1-pbinom(100, 120, 0.85)
1-pbinom(100, 110, 0.85)
dbinom(120, 120, 0.85)
dbinom(110, 120, 0.85)
dbinom(111, 120, 0.85)
1-pbinom(110, 120, 0.85)
install.packages('wooldridge')
library(wooldridge)
updateR()
install.packages('installr')
library(statsr)
install.packages("statsr")
############################################################
#     Foundation to Strategic Business Analytics           #
#       Module 3 - Understanding causes and consequences	 #
# 	Author: Nicolas Glady & Pauline Glikman                #
#                   ESSEC BUSINESS SCHOOL                  #
############################################################
############################################################
####     EXAMPLE NÂ°1 - CREDIT SCORING                   ###
############################################################
# Set your directory to the folder where you have downloaded the Credit Scoring dataset
setwd("/Users/chloe/Documents/GitHub/Coursera/Foundations of strategic business analytics/Week2")
# To clean up the memory of your current R session run the following line
rm(list=ls(all=TRUE))
data=read.table('DATA_3.01_CREDIT.csv',sep=',',header=TRUE) # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
hist(data$Rating) # Produce a histogram of the credit scores
cor(data[,c(1:5,10)]) # Compute the correlation between all the numerical variables of the sample
linreg=lm(Rating~.,data=data) # Estimate a linear regression model of Rating as a function of everything else.
cor(linreg$fitted.values,data$Rating) # Computes the correlation between the fitted values and the actual ones
plot(data$Rating,linreg$fitted.values) # Plot the fitted values vs. the actual ones
summary(linreg) # Reports the results of the regression
plot(data$Balance,data$Rating) # Allows to visualize the relationship between Balance and Rating
plot(data$Income,data$Rating) # Allows to visualize the relationship between Income and Rating
rm(list=ls(all=TRUE))
rm(list=ls(all=TRUE))
datatot=read.table('DATA_3.02_HR2.csv', header = T,sep=',')
str(datatot) # The str() function shows the structure of your dataset and details the type of variables that it contains
View(data)
View(datatot)
table(datatot$left) # look at the frequencies for the left variable
table(datatot$left)/nrow(datatot) # look at percentages for the left variable
hist(datatot$left) # alternatively, plot a histogram
cor(datatot) # Let's check out the correlations
?glm
logreg = glm(left ~ ., family=binomial(logit), data=datatot) # Estimate the drivers of attrition
summary(logreg)
?lm
summary(lm(left ~ ., data = datatot))
?glm
hist(logreg$fitted.values) # See the proportion of employee attrition according to the model
test <- lm(left ~ ., data = datatot)
test$fitted.values
hist(test$fitted.values)
cor(logreg$fitted.values,datatot$left) # Assess the correlation between estimated attrition and actual
sum((logreg$fitted.values<=cutoff)&(datatot$left==0))/sum(datatot$left==0) # Compute the percentage of correctly classified employees who stayed
cutoff=.3 # Cutoff to determine when P[leaving] should be considered as a leaver or not. Note you can play with it...
sum((logreg$fitted.values<=cutoff)&(datatot$left==0))/sum(datatot$left==0) # Compute the percentage of correctly classified employees who stayed
sum((logreg$fitted.values>cutoff)&(datatot$left==1))/sum(datatot$left==1) # Compute the percentage of correctly classified employees who left
mean((logreg$fitted.values>cutoff)==(datatot$left==1)) # Compute the overall percentage of correctly classified employees
summary(logreg) # Report the results of the logistic regression
plot(datatot$TIC,datatot$left,main= "Time and Employee Attrition", ylab="Attrition", xlab= "Time spent")
tempdata=datatot
aggbTimeRank=aggregate(left~ TIC, data=tempdata, FUN=mean) # We compute the average attrition rate for each value of TIC
aggbTimeRank
library(dplyr)
tempdata %>% group_by(TIC) %>% summarise(mean(left))
plot(aggbTimeRank$TIC,aggbTimeRank$left,main= "Time and Employee Attrition", ylab="Average Attrition Rate", xlab= "Time spent")
cntbTimeRank=aggregate(left~ TIC, data=tempdata, FUN=length) # We compute the number of employees for each value of TIC
symbols(aggbTimeRank$TIC,aggbTimeRank$left,circles=cntbTimeRank$left, inches=.75, fg="white", bg="red",main= "Time and Employee Attrition", ylab="Average Attrition Rate", xlab= "Time spent") # we
table(tempdata$S)
tempdata$rankSatis = round(rank(-tempdata$S)/600)
table(tempdata$rankSatis)
nrow(tempdata)
aggbSatisRank = aggregate(left~ rankSatis, data=tempdata, FUN=mean) # We compute the average attrition rate for each category
cntbSatisRank = aggregate(left~ rankSatis, data=tempdata, FUN=length) # We compute the number of employees for each value of TIC
symbols(aggbSatisRank$rankSatis,aggbSatisRank$left,circles=cntbSatisRank$left, inches=.2, fg="white", bg="red",main= "Satisfaction and Employee Attrition", ylab="Average Attrition Rate", xlab= "Rank of Satisfaction")
size = cntbTimeRank$left
radius = sqrt(size / pi)
symbols(x = aggbTimeRank$TIC, y = aggbTimeRank$left,
circles = radius, inches = .75, fg = "white", bg = "red",
main =  "Time and Employee Attrition",
ylab = "Average Attrition Rate", xlab =  "Time spent")
setwd("/Users/chloe/Documents/GitHub/Coursera/Foundations of strategic business analytics/Week2")
# To clean up the memory of your current R session run the following line
rm(list=ls(all=TRUE))
data=read.table('DATA_3.01_CREDIT.csv',sep=',',header=TRUE) # The function read.table enables us to read flat files such as .csv files
linreg=lm(Rating~.,data=data) # Estimate a linear regression model of Rating as a function of everything else.
summary(linreg) # Reports the results of the regression
View(summary(linreg))
summary(linreg)
linreg1 <- lm(Rating ~ Income+Cards+MarriedYes, data=data)
View(data)
linreg1 <- lm(Rating ~ Income+Cards+Married, data=data)
summary(linreg1)
rm(list=ls(all=TRUE))
# Let's load our dataset and call it datatot
datatot=read.table('DATA_3.02_HR2.csv', header = T,sep=',')
logreg = glm(left ~ ., family=binomial(logit), data=datatot) # Estimate the drivers of attrition
cutoff=.5 # Cutoff to determine when P[leaving] should be considered as a leaver or not. Note you can play with it...
sum((logreg$fitted.values<=cutoff)&(datatot$left==0))/sum(datatot$left==0) # Compute the percentage of correctly classified employees who stayed
sum((logreg$fitted.values>cutoff)&(datatot$left==1))/sum(datatot$left==1) # Compute the percentage of correctly classified employees who left
mean((logreg$fitted.values>cutoff)==(datatot$left==1)) # Compute the overall percentage of correctly classified employees
summary(logreg)
