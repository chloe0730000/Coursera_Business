flights %>% select(year, month, day)
flights %>% select(-year)
flights$carrier %>% select(contains("UA"))
flights %>% select(contains("UA"))
names(flights)
mutate(unite(flights, date, year, month, day, sep = "-"))
a <- mutate(unite(flights, date, year, month, day, sep = "-"))
View(a)
library(lubridate)
a <- mutate(unite(flights, date, year, month, day, sep = "-"))
a$date <- as.Date(a$date)
View(a)
a <- mutate(unite(flights, date, year, month, day, sep = "-"),
flight_new <- flights$flight/100)
a$date <- as.Date(a$date)
View(a)
a <- mutate(unite(flights, date, year, month, day, sep = "-"),
flight_new = flight/100)
a$date <- as.Date(a$date)
View(a)
x <- 1:10
cumsum(x)
flights
flights %>% group_by(carrier) %>% summarize(count = n())
shiny::runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/ref3')
shiny::runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('~/Documents/GitHub/DataCamp_R/Shiny_dashboard/Case_studies')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
shiny::runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard/Build Interactive Dashboard/fluidPage/v3')
shiny::runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard')
runApp('Desktop/Temp_Shiny')
LETTERS
sample(LETTERS, 5)
runApp('Desktop/Temp_Shiny')
?selectInput
runApp('Desktop/Temp_Shiny')
region_list <- c("APAC", "NAM", "LATAM", "EMEA")
lapply(region_list, function(x){paste("Total Mailbox -", x)})
runApp('Desktop/Temp_Shiny')
strong(paste0('Hi, this is output B#', i))
runApp('Desktop/Temp_Shiny')
mtcars
mtcars.info()
mtcars.describe()
sum(mtcars$cyl)
sum(is.na(mtcars$cyl))
unique(mtcars$vs)
paste0("m_",1)
fluidRow(
box(valueBoxOutput("mailbox_a", width = 6),
title="Total Mailbox - APAC" ,width = 3,background = 'teal'),
box(valueBoxOutput("mailbox_n", width = 6),
title="Total Mailbox - NAM" ,width = 3,background = 'teal'),
box(valueBoxOutput("mailbox_l", width = 6),
title="Total Mailbox - LATAM" ,width = 3,background = 'teal'),
box(valueBoxOutput("mailbox_e", width = 6),
title="Total Mailbox - EMEA" ,width = 3,background = 'teal')
)
fluidRow(
lapply(c("APAC", "NAM", "LATAM", "EMEA"), function(i){box(valueBoxOutput(paste0("m_",i), width=6),
title = paste("Total Mailbox -", i), width = 3, background = "teal")})
)
res <- lapply(1:5, function(i) input[[paste0('a', i)]])
?setNames
runApp('Desktop/Temp_Shiny')
installed.packages("shinydashboardPlus")
library(shinydashboardPlus)
installed.packages("shinydashboardPlus")
fluidRow(
lapply(0:1, function(i){box(valueBoxOutput(paste0("m_",i), width=6),
title = paste("Total Mailbox -", i), width = 3, background = "teal")})
)
lapply(1:2, function(x){
output[[paste0('m_',x)]] <- renderText({
data <- mtcars
data <- mtcars %>% filter(vs==x)
return(dim(data)[1])
})
})
output$mailbox_n <- renderText({
data <- data_m
data <- data %>% filter(!is.na(MailboxSizeGB), Region=="NAM")
return(dim(data)[1])
})
runApp('Documents/GitHub/DataCamp_R/Shiny_dashboard')
mtcars
str(mtcars)
sum(is.na(mpg))
sum(is.na(mtcars$mpg))
sum(is.na(mtcars$cyl))
sum(is.na(mtcars$vs))
sum(is.na(mtcars$carb))
sum(is.na(mtcars$gear))
sum(is.na(mtcars$hp))
library(readxl)
data <- read.csv("~/Documents/GitHub/DataCamp_R/Datasets/census.csv")
str(data)
sum(is.na(data$JAN))
sum(is.na(data$OCT))
sum(is.na(data))
library(readxl)
data <- load("~/Documents/GitHub/DataCamp_R/Datasets/weather.rds")
weather <- readRDS("~/Documents/GitHub/DataCamp_R/Datasets/weather.rds")
sum(is.na(data))
library(readr)
data <- read.csv("~/Downloads/GitHub/DataCamp_R/Datasets/students.csv")
library(readr)
data <- read.csv("~/Downloads/GitHub/DataCamp_R/Datasets/students.csv")
library(readr)
data <- read.csv("~/Downloads/GitHub/DataCamp_R/Datasets/students.csv")
library(readr)
data <- read.csv("~/Documents/GitHub/DataCamp_R/Datasets/students.csv")
View(data)
unique(data$address)
unique(data$Pstatus)
unique(data$Medu)
data$Medu[data$Medu=="4"] <- NA
data$Medu[data$Medu=="3"] <- "abc"
unique(data$Medu)
if (is.na(data$Medu)){
data$Medu[is.na(data$Medu)] <- 4
} else if(data$Medu=="abc"){
data$Medu[is.na(data$Medu)] <- 4
} else{
data$Medu <- data$Medu
}
unique(data$Medu)
# Learn R program to apply a function for each row in r data frame
# R Data Frame
celebrities = data.frame(name = c("Andrew", "Mathew", "Dany", "Philip", "John", "Bing", "Monica"),
age = c(28, 23, 49, 29, 38, 23, 29),
income = c(25.2, 10.5, 11, 21.9, 44, 11.5, 45))
# R function
f = function(x, output) {
# x is the row of type Character
# access element in first column
name = x[1]
# access element in second column
income = x[3]
#your code to process x
cat(name, income, "\n")
}
#apply(X, MARGIN, FUN, …)
apply(celebrities, 1, f)
celebrities = data.frame(name = c("Andrew", "Mathew", "Dany", "Philip", "John", "Bing", "Monica"),
age = c(28, 23, 49, 29, 38, 23, 29),
income = c(25.2, 10.5, 11, 21.9, 44, 11.5, 45))
# R function
f = function(x$name, output) {
# x is the row of type Character
# access element in first column
name = x[1]
# access element in second column
income = x[3]
#your code to process x
cat(name, income, "\n")
}
#apply(X, MARGIN, FUN, …)
apply(celebrities, 1, f)
data <- apply(data$Medu, 1, function(x){
if(is.na(x$Medu)){
x$Medu[is.na(x$Medu)] <- 4
}else if(x$Medu == "abc"){
x$Medu[x$Medu=="abc"] <- 3
}else{
x$Medu
}
})
data <- apply(data, 1, function(x){
if(is.na(x$Medu)){
x$Medu[is.na(x$Medu)] <- 4
}else if(x$Medu == "abc"){
x$Medu[x$Medu=="abc"] <- 3
}else{
x$Medu
}
})
data$Medu <- apply(data$Medu, 1, function(x){
if(is.na(x)){
x[is.na(x)] <- 4
}else if(x == "abc"){
x[x=="abc"] <- 3
}else{
x
}
})
data$Medu <- ifelse(is.na(data$Medu), 4, data$Medu)
unique(data$Medu)
data$Medu <- ifelse(is.na(data$Medu), 4, data$Medu)
data$Medu <- ifelse(data$Medu=="abc", 3, data$Medu)
unique(data$Medu)
data$Medu[data$Medu=="4"] <- "10 GB"
data$Medu[data$Medu=="3"] <- "1000 KB"
data$Medu <- ifelse(data$Medu=="10 GB", str_extract(data$Medu, [0-9]{1,5}), data$Medu)
data$Medu <- ifelse(data$Medu=="10 GB", str_extract(data$Medu, "[0-9]{1,5}"), data$Medu)
library(stringr)
data$Medu <- ifelse(data$Medu=="10 GB", str_extract(data$Medu, "[0-9]{1,5}"), data$Medu)
unique(data$Medu)
data$Medu <- ifelse(data$Medu=="10 GB", trimws(str_extract(data$Medu, "[0-9]{1,5}")), data$Medu)
unique(data$Medu)
data$Medu <- ifelse(data$Medu=="1000 KB", trimws(str_extract(data$Medu, "[0-9]{1,5}")), data$Medu)
unique(data$Medu)
str_extract("0.1 GB", [0-9]{1,5})
str_extract("0.1 GB", "[0-9]{1,5}")
grep(pattern = "(0\\.[0-9]+)", "0.1 GB" ,value = T)
grep(pattern = "(0\\.[0-9]{1,4})", "0.1 GB" ,value = T)
grep(pattern = "(\\.[0-9]{1,4})", "0.1 GB" )
grep(pattern = "([0-9]{1,4})?(\\.[0-9]{1,4})", "0.1 GB" )
regexpr("([0-9]{1,4})?(\\.[0-9]{1,4})", "0.1 GB")
str_extract("24 GB", "\\d+\\.*\\d*")
str_extract("24.15 GB", "\\d+\\.*\\d+")
str_extract("24.15 GB", "\\d+\\.*\\d*")
unique(data$Medu)
data$Medu[data$Medu=="10"] <- "0.15 GB"
data$Medu[data$Medu=="1000"] <- "0.1 GB"
unique(data$Medu)
data$Medu <- ifelse(data$Medu=="0.15 GB", trimws(str_extract(data$Medu, "\\d+\\.*\\d+")), data$Medu)
data$Medu <- ifelse(data$Medu=="0.1 GB", trimws(str_extract(data$Medu, "\\d+\\.*\\d+")), data$Medu)
unique(data$Medu)
R.version
install.packages("swirl")
library(swirl)
install_course("Advance R Programming")
swirl::install_course("Advanced R Programming")
swirl()
x <- runif(1, 0, 10)
x
if(x > 3) {
y <- 10
} else {
y <- 0
}
y
x <- matrix(1:6, 2, 3)
x
nrow(x)
ncol(x)
for(i in seq_len(nrow(x))){print(i)}
for(i in seq_len(nrow(x))) {
for(j in seq_len(ncol(x))) {
print(x[i, j])
}
}
for(i in seq_len(ncol(x))){print(i)}
x
gwd
getwd
getwd()
if(!file.exists("data/2016-07-20.csv.gz")) {
download.file("http://cran-logs.rstudio.com/2016/2016-07-20.csv.gz",
"data/2016-07-20.csv.gz")
}
if(!file.exists("data/2016-07-20.csv.gz")) {
download.file("http://cran-logs.rstudio.com/2016/2016-07-20.csv.gz",
"data/2016-07-20.csv.gz")
}
cran <- read_csv("data/2016-07-20.csv.gz", col_types = "ccicccccci")
library(readr)
library(dplyr)
cran <- read_csv("data/2016-07-20.csv.gz", col_types = "ccicccccci")
cran <- read_csv("data/2016-07-20.csv.gz", col_types = "ccicccccci")
cran <- read_csv("data/2016-07-20.csv.gz", col_types = "ccicccccci")
cran %>% filter(package == "filehash") %>% nrow
swirl()
Sys.Date()
mean(c(2,4,5))
submit()
boring_function('My first function!')
boring_function()
boring_function
submit()
my_mean(c(4,5,10))
submit()
remainder(5)
remainder(11,5)
remiander(divisor=11, num=5)
remainder(divisor = 11, num=5)
remainder(4, div=2)
args(remainder)
submit()
submit()
evaluate(sd, c(1.4, 3.6, 7.9, 8.8))
evaluate(function(x){x+1},6)
evaluate(function(x){x[1]}, c(8,4,0))
evaluate(function(x){x[3]}, c(8,4,0))
evaluate(function(x){x[length(x)]}, c(8,4,0))
?paste
paste("Programming", "is", "fun!")
submit()
paste("START", "hi")
submit()
telegram("Good", "morning")
submit()
mad_libs("US", "car")
submit()
"I" %p% "love" %p% "R!"
swirl()
Sys.Date()
swilr()
swirl()
adder_maker <- function(n){
function(x){
n + x
}
}
add2 <- adder_maker(2)
add3 <- adder_maker(3)
add2(5)
add3(5)
library(purrr)
map_chr(c(5, 4, 3, 2, 1), function(x){
c("one", "two", "three", "four", "five")[x]
})
map_lgl(c(1, 2, 3, 4, 5), function(x){
x > 3
})
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
set.seed(1234)
library(ggplot2)
library(lattice)
set.seed(1234)
library(ggplot2)
library(lattice)
dbern(1,0.85)
dbinom(100, 120, 0.85)
pbinom(100, 120, 0.85)
1-pbinom(100, 120, 0.85)
1-pbinom(100, 110, 0.85)
dbinom(120, 120, 0.85)
dbinom(110, 120, 0.85)
dbinom(111, 120, 0.85)
1-pbinom(110, 120, 0.85)
install.packages('wooldridge')
library(wooldridge)
updateR()
install.packages('installr')
library(statsr)
install.packages("statsr")
setwd("/Users/chloe/Documents/GitHub/Coursera/Foundations of strategic business analytics/Week3")
# To clean up the memory of your current R session run the following line
rm(list=ls(all=TRUE))
# Let's load our dataset and call it data
dataold=read.table('DATA_3.01_CREDIT.csv',sep=',',header=TRUE) # The function read.table enables us to read flat files such as .csv files
datanew=read.table('DATA_4.01_CREDIT2.csv',sep=',',header=TRUE) # The function read.table enables us to read flat files such as .csv files
summary(datanew) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
?predict
# Set your directory to the folder where you have downloaded the Insurance 2 dataset
setwd("/Users/chloe/Documents/GitHub/Coursera/Foundations of strategic business analytics/Week3")
# To clean up the memory of your current R session run the following line
rm(list=ls(all=TRUE))
# Let's load our dataset and call it data
dataold=read.table('DATA_3.01_CREDIT.csv',sep=',',header=TRUE) # The function read.table enables us to read flat files such as .csv files
datanew=read.table('DATA_4.01_CREDIT2.csv',sep=',',header=TRUE) # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(datanew) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(datanew) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
linreg=lm(Rating~.,data=dataold) # Estimate a linear regression model of Rating as a function of everything else.
predcreditscore = predict(linreg,newdata=datanew,type="response")
cor(linreg$fitted.values,dataold$Rating) # Computes the correlation between the fitted values and the actual ones
plot(dataold$Rating,linreg$fitted.values) # Plot the fitted values vs. the actual ones
cor(predcreditscore,datanew$Rating) # Computes the correlation between the fitted values and the actual ones
plot(datanew$Rating,predcreditscore) # Plot the fitted values vs. the actual ones
rm(list=ls(all=TRUE))
dataold=read.table('DATA_3.02_HR2.csv', header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
datanew=read.table('DATA_4.02_HR3.csv', header = T,sep=',') # The new dataset on which we want to make the prediction
summary(datanew) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
logreg = glm(left ~ ., family=binomial(logit), data=dataold) # Estimate the drivers of attrition
probaToLeave=predict(logreg,newdata=datanew,type="response") # Make predictions on the out-of-sample data
predattrition = data.frame(probaToLeave) # Structure the prediction output in a table
View(predattrition) # View the predattrition dataframe
predattrition$performance=datanew$LPE # Add a column to the predattrition dataframe containing the performance
plot(predattrition$probaToLeave,predattrition$performance)
predattrition$priority=predattrition$performance*predattrition$probaToLeave
orderpredattrition=predattrition[order(predattrition$priority,decreasing = TRUE),]
rm(list=ls(all=TRUE))
data=read.table('DATA_4.03_MNT.csv',sep=',',header=TRUE)
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
# to clean up the memory of your current R session run the following line
rm(list=ls(all=TRUE))
data=read.table('DATA_4.03_MNT.csv',sep=',',header=TRUE)
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
linregmodel = lm(lifetime~.-broken,data=data)  # Build a linear regression model
summary(linregmodel) # The summary() function shows the output of your model
install.packages("survival") # Install the survival package to your computer
library(survival) # Load the survival package
?Surv
View(data)
dependantvars = Surv(data$lifetime, data$broken) # choose the dependant variables to be used in the survival regression model with the Surv() function
survreg = survreg(dependantvars~pressureInd+moistureInd+temperatureInd+team+provider, dist="gaussian",data=data) # Create your survival regression model
summary(survreg)  # The summary() function shows the output of your model
Ebreak=predict(survreg, newdata=data, type="quantile", p=.5) # Make predictions based on the model. Here we estimate the median lifetime as the expected moment of "death"
Forecast=data.frame(Ebreak) # Create a dataframe to store the ouput of Ebreak
View(Forecase)
View(Forecast)
Ebreak1=predict(survreg, newdata=data, type="response") # Make predictions based on the model. Here we estimate the median lifetime as the expected moment of "death"
Forecast$Ebreak1 = Ebreak1
View(Forecast)
dependantvars = Surv(data$lifetime, data$broken) # choose the dependant variables to be used in the survival regression model with the Surv() function
survreg = survreg(dependantvars~pressureInd+moistureInd+temperatureInd+team+provider, dist="gaussian",data=data) # Create your survival regression model
summary(survreg)  # The summary() function shows the output of your model
Ebreak=predict(survreg, newdata=data, type="quantile", p=.5) # Make predictions based on the model. Here we estimate the median lifetime as the expected moment of "death"
Forecast=data.frame(Ebreak) # Create a dataframe to store the ouput of Ebreak
Forecast$lifetime=data$lifetime  # Add a column in the Forecast dataframe indicating the lifetime of the piece
Forecast$broken=data$broken # Add a column in the Forecast dataframe indicating whether or not the piece is broken
View(Forecast)
Forecast$RemainingLT=Forecast$Ebreak-data$lifetime # Computed Expected Remaining Lifetime
View(Forecast)
Forecast=Forecast[order(Forecast$RemainingLT),] # Order the elements by Expected Remaining Lifetime
View(Forecast)
ActionsPriority=Forecast[Forecast$broken==0,] # And keep only those who are not broken yet
View(ActionsPriority) # View the output and take actions!
rm(list=ls(all=TRUE))
# Let's load our dataset
dataold=read.table('DATA_3.02_HR2.csv', header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
datanew=read.table('DATA_4.02_HR3.csv', header = T,sep=',') # The new dataset on which we want to make the prediction
str(datanew) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(datanew) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
logreg = glm(left ~ ., family=binomial(logit), data=dataold) # Estimate the drivers of attrition
probaToLeave=predict(logreg,newdata=datanew,type="response") # Make predictions on the out-of-sample data
predattrition = data.frame(probaToLeave) # Structure the prediction output in a table
View(predattrition) # View the predattrition dataframe
View(datanew)
datanew$ID = seq.int(nrow(datanew))
View(datanew)
predattrition$ID = datanew$ID
View(predattrition)
predattrition %>% arrange(desc(probaToLeave))
library(dplyr)
predattrition = predattrition %>% arrange(desc(probaToLeave))
View(predattrition)
predattrition = predattrition %>% arrange(probaToLeave)
View(predattrition)
predattrition$performance=datanew$LPE # Add a column to the predattrition dataframe containing the performance
View(predattrition) # View the predattrition dataframe
predattrition %>% filter(LPE>0.9)
predattrition = predattrition %>% filter(performance>0.9)
View(predattrition) # View the predattrition dataframe
rm(list=ls(all=TRUE))
# Let's load the data
data=read.table('DATA_4.03_MNT.csv',sep=',',header=TRUE)
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
linregmodel = lm(lifetime~.-broken,data=data)  # Build a linear regression model
summary(linregmodel) # The summary() function shows the output of your model
install.packages("survival") # Install the survival package to your computer
library(survival) # Load the survival package
dependantvars = Surv(data$lifetime, data$broken) # choose the dependant variables to be used in the survival regression model with the Surv() function
survreg = survreg(dependantvars~pressureInd+moistureInd+temperatureInd+team+provider, dist="gaussian",data=data) # Create your survival regression model
summary(survreg)  # The summary() function shows the output of your model
install.packages("survival")
summary(survreg)  # The summary() function shows the output of your model
survreg1 = survreg(dependantvars~pressureInd+moistureInd+temperatureInd, dist="gaussian",data=data) # Create your survival regression model
summary(survreg1)  # The summary() function shows the output of your model
Ebreak=predict(survreg, newdata=data, type="quantile", p=.5) # Make predictions based on the model. Here we estimate the median lifetime as the expected moment of "death"
Forecast=data.frame(Ebreak) # Create a dataframe to store the ouput of Ebreak
Forecast$lifetime=data$lifetime  # Add a column in the Forecast dataframe indicating the lifetime of the piece
Forecast$broken=data$broken # Add a column in the Forecast dataframe indicating whether or not the piece is broken
Forecast$RemainingLT=Forecast$Ebreak-data$lifetime # Computed Expected Remaining Lifetime
View(Forecast)
View(Forecast1)
Forecast1 = Forecast %>% arrange(desc(RemainingLT))
View(Forecast1)
Forecast$ID = seq.int(nrow(Forecast))
Forecast = Forecast %>% arrange(desc(RemainingLT))
View(Forecast)
rm(list=ls(all=TRUE))
# Let's load our dataset
dataold=read.table('DATA_3.02_HR2.csv', header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
datanew=read.table('DATA_4.02_HR3.csv', header = T,sep=',') # The new dataset on which we want to make the prediction
str(datanew) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(datanew) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
logreg = glm(left ~ ., family=binomial(logit), data=dataold) # Estimate the drivers of attrition
probaToLeave=predict(logreg,newdata=datanew,type="response") # Make predictions on the out-of-sample data
predattrition = data.frame(probaToLeave) # Structure the prediction output in a table
View(predattrition) # View the predattrition dataframe
datanew$ID = seq.int(nrow(datanew))
View(datanew)
predattrition$ID = datanew$ID
View(predattrition)
library(dplyr)
predattrition = predattrition %>% arrange(probaToLeave)
predattrition = data.frame(probaToLeave) # Structure the prediction output in a table
View(predattrition) # View the predattrition dataframe
datanew$ID = seq.int(nrow(datanew))
View(datanew)
predattrition$ID = datanew$ID
predattrition$performance=datanew$LPE # Add a column to the predattrition dataframe containing the performance
View(predattrition)
library(dplyr)
predattrition = predattrition %>% arrange(probaToLeave)
predattrition1 = predattrition %>% filter(performance>0.9)
View(predattrition1) # View the predattrition dataframe
View(predattrition)
